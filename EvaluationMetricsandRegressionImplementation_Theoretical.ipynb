{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayesha765/pwAssign/blob/main/EvaluationMetricsandRegressionImplementation_Theoretical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm6M7qX8IhXl"
      },
      "source": [
        "---\n",
        "\n",
        "# THEORETICAL QUESTION\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM3-7OqLIhXz"
      },
      "source": [
        "**1. What does R-squared represent in a regression model?**\n",
        "\n",
        "   - **R-squared**, or the coefficient of determination, measures how well the independent variables explain the variability of the dependent variable.\n",
        "\n",
        "   - A value of **1** means the model explains 100% of the variability in the data, while a value of 0 means it explains none.\n",
        "\n",
        "   - For example, if \\( R Square = 0.75 \\), it means 75% of the variation in the target variable is explained by the predictors, while the remaining 25% is due to unknown or unmodeled factors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kGNwAmuIhX3"
      },
      "source": [
        "**2. What are the assumptions of linear regression?**\n",
        "\n",
        "   - Linear regression relies on the following assumptions for accurate results:\n",
        "\n",
        "   1. **Linearity**: The relationship between predictors and the target is linear.\n",
        "\n",
        "   2. **Independence**: Observations are independent (e.g., no correlation between consecutive errors in time series).\n",
        "\n",
        "   3. **Homoscedasticity**: The residuals (errors) have constant variance across all levels of the independent variables.\n",
        "\n",
        "   4. **Normality of residuals**: Residuals should be normally distributed to ensure valid hypothesis testing.\n",
        "   \n",
        "   5. **No multicollinearity**: Predictors should not be highly correlated with each other, as it makes coefficients unreliable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQm8gImYIhX5"
      },
      "source": [
        "**3. What is the difference between R-squared and Adjusted R-squared?**\n",
        "\n",
        "   - **R-squared** increases whenever a new predictor is added to the model, even if the predictor doesn't improve the model.\n",
        "\n",
        "   - **Adjusted R-squared** accounts for the number of predictors in the model. It increases only if the added predictors improve the model performance.\n",
        "   \n",
        "   - For example, if you add a predictor that doesn’t contribute much, Adjusted \\( R^2 \\) may decrease, while \\( R^2 \\) will still increase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hmQc4laIhX9"
      },
      "source": [
        "**4. Why do we use Mean Squared Error (MSE)?**\n",
        "\n",
        "   - **MSE** measures the average squared differences between the predicted and actual values.\n",
        "\n",
        "   - Squaring the errors penalizes larger errors more than smaller ones, making MSE sensitive to significant deviations.\n",
        "   \n",
        "   - MSE is also easier to optimize in mathematical models (like gradient descent) because it has a smooth derivative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AYI1yYcIhYA"
      },
      "source": [
        "**5. What does an Adjusted R-squared value of 0.85 indicate?**\n",
        "   - An Adjusted \\( R Square \\) value of **0.85** means that 85% of the variance in the target variable is explained by the predictors, adjusted for the number of predictors.\n",
        "\n",
        "   - It shows that the model fits well but has considered the complexity (number of predictors) to avoid overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY8flf1bIhYC"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7zSI1zgIhYE"
      },
      "source": [
        "**6. How do we check for normality of residuals in linear regression?**\n",
        "\n",
        "   To ensure residuals follow a normal distribution, we can:\n",
        "\n",
        "   1. Plot a **histogram** of residuals to visually check for normality.\n",
        "\n",
        "   2. Use a **Q-Q plot** (quantile-quantile plot), where points should follow a straight line if residuals are normal.\n",
        "\n",
        "   3. Perform statistical tests like the **Shapiro-Wilk test** or **Jarque-Bera test**.\n",
        "   \n",
        "   - Normal residuals are crucial for valid confidence intervals and p-values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdYWzcPRIhYF"
      },
      "source": [
        " **7. What is multicollinearity, and how does it impact regression?**\n",
        "   - **Multicollinearity** occurs when two or more predictors are highly correlated, making it hard to determine their individual effects on the target variable.\n",
        "   - Impact:\n",
        "     - Coefficient estimates become unstable.\n",
        "     - Predictions might remain accurate, but the interpretation of individual predictors becomes unreliable.\n",
        "   - For example, if you use both \"age\" and \"years of experience\" in a model, they may show multicollinearity as they are often related."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sca26a5QIhYH"
      },
      "source": [
        "**8. What is Mean Absolute Error (MAE)?**\n",
        "\n",
        "   - **MAE** is the average of the absolute differences between predicted and actual values.\n",
        "\n",
        "   - It’s simpler than MSE and is less sensitive to outliers because it doesn’t square errors.\n",
        "   \n",
        "   - For example, if the predictions are off by 5, 10, and 15 units, the MAE will be (5 + 10 + 15)\\3 = 10 ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShI91UiVIhYJ"
      },
      "source": [
        "**9. What are the benefits of using an ML pipeline?**\n",
        "\n",
        "   - **Consistency**: Ensures the same preprocessing steps are applied during training and prediction.\n",
        "\n",
        "   - **Automation**: Reduces manual effort by automating tasks like scaling, encoding, and feature selection.\n",
        "\n",
        "   - **Efficiency**: Streamlines workflows, saving time and effort.\n",
        "\n",
        "   - **Reproducibility**: Makes it easy to reproduce results, especially with large datasets.\n",
        "   \n",
        "   - Example: A pipeline might standardize data, handle missing values, and train a regression model in one sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUBI4YA0IhYK"
      },
      "source": [
        "**10. Why is RMSE considered more interpretable than MSE?**\n",
        "\n",
        "   - **MSE** uses squared units of the target variable, making it hard to interpret.\n",
        "   \n",
        "   - **RMSE** takes the square root of MSE, bringing the error back to the same unit as the target variable, which is more intuitive for decision-making."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR2KugnwIhYM"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNYFDiUPIhYM"
      },
      "source": [
        "**11. What is pickling in Python, and how is it useful in ML?**\n",
        "\n",
        "   - **Pickling** converts a Python object (like a trained model) into a binary format for saving or transferring.\n",
        "\n",
        "   - In ML, pickling helps:\n",
        "\n",
        "     - Save trained models to avoid retraining.\n",
        "     \n",
        "     - Share models with others or deploy them for predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5xX5ZEEIhYT"
      },
      "source": [
        "**12. What does a high R-squared value mean?**\n",
        "\n",
        "   - A high Rsquare value (close to 1) means the model explains a large portion of the variability in the dependent variable.\n",
        "   \n",
        "   - However, it doesn’t guarantee a good model; overfitting can inflate Rsquare. Always check residuals and other metrics like Adjusted Rsquare and RMSE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ3W9HRaIhYU"
      },
      "source": [
        "**13. What happens if linear regression assumptions are violated?**\n",
        "\n",
        "   - If assumptions are violated:\n",
        "\n",
        "     - Predictions might still work, but the statistical inferences (e.g., p-values, confidence intervals) will be invalid.\n",
        "\n",
        "     - Violations like heteroscedasticity or non-normality can lead to biased or inefficient coefficient estimates.\n",
        "     \n",
        "   - Example: If residuals are not normally distributed, the p-values for coefficients might be misleading.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "954LIshHIhYV"
      },
      "source": [
        "**14. How can we address multicollinearity in regression?**\n",
        "\n",
        "   - **Remove correlated predictors**: Drop one of the highly correlated variables.\n",
        "\n",
        "   - Use **Regularization** techniques like Lasso or Ridge regression to handle multicollinearity.\n",
        "   \n",
        "   - Apply **PCA (Principal Component Analysis)** to reduce dimensions while preserving most of the variance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxrbHeESIhYW"
      },
      "source": [
        "**15. How can feature selection improve model performance in regression analysis?**\n",
        "\n",
        "   - **Feature selection** identifies and retains only the most relevant predictors, improving:\n",
        "\n",
        "     - Model accuracy by reducing noise.\n",
        "\n",
        "     - Interpretability by focusing on key drivers of the target variable.\n",
        "     \n",
        "     - Efficiency by lowering computational costs and training time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-soEJI7IhYY"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHy73uwOIhYZ"
      },
      "source": [
        "**16. How is Adjusted R-squared calculated?**\n",
        "   - Adjusted R-squared is calculated using the following formula:\n",
        "\n",
        "   - Adjusted R-squared = 1 - [(1 - R-squared) * (n - 1) / (n - k - 1)]\n",
        "     Where:\n",
        "     -  R-sq: Regular R-squared value.\n",
        "     -  n : Number of observations.\n",
        "     -  k : Number of predictors.\n",
        "\n",
        "   - It adjusts \\( R-sq \\) for the number of predictors, penalizing the addition of irrelevant features. Unlike \\( R-sq \\), it decreases if the added predictors don’t improve the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oST6Ya5BIhYb"
      },
      "source": [
        "**17. Why is MSE sensitive to outliers?**\n",
        "\n",
        "   - **MSE (Mean Squared Error)** squares the residuals (errors), which amplifies the effect of larger errors.\n",
        "\n",
        "   - Mean Squared Error (MSE) is sensitive to outliers because it squares the errors, meaning large deviations from the predicted values (like those caused by outliers) are significantly amplified, disproportionately impacting the overall MSE value compared to smaller errors; essentially, a single outlier can significantly inflate the MSE due to its squared contribution.\n",
        "     \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpQ1cqeOIhYh"
      },
      "source": [
        "**18. What is the role of homoscedasticity in linear regression?**\n",
        "\n",
        "   - Homoscedasticity means that the residuals (errors) have constant variance across all levels of the predictors.\n",
        "\n",
        "   - Importance:\n",
        "     - Ensures valid statistical tests (e.g., t-tests, F-tests).\n",
        "     - Prevents over- or under-estimation of coefficients.\n",
        "   - Violations (heteroscedasticity) can lead to biased standard errors, affecting p-values and confidence intervals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yia4tIJGIhYj"
      },
      "source": [
        " **19. What is Root Mean Squared Error (RMSE)?**\n",
        "\n",
        "   - RMSE is the square root of MSE:\n",
        "\n",
        "   - Root Mean Squared Error (RMSE) is a metric used to measure the average difference between predicted and actual values in a regression model. It is calculated by taking the square root of the average of squared errors.\n",
        "\n",
        "   -  RMSE gives an idea of how well the model's predictions match the actual values, with lower values indicating better performance. It is sensitive to larger errors due to squaring the differences.\n",
        "   \n",
        "   - It represents the standard deviation of prediction errors, providing an interpretable measure of model accuracy in the same units as the target variable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MARuaDZTIhYl"
      },
      "source": [
        "**20. Why is pickling considered risky?**\n",
        "\n",
        "   - Pickling can execute arbitrary code during deserialization, making it a security risk if files are tampered with.\n",
        "\n",
        "   - Risks include:\n",
        "\n",
        "     - **Code injection**: Malicious code can be embedded in a pickled file.\n",
        "\n",
        "     - **Compatibility issues**: Pickled files might not work across different Python versions.\n",
        "     \n",
        "   - Always verify the source of pickled files before loading them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieGwvvwCIhYm"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rptBJVZDIhYn"
      },
      "source": [
        "**21. What alternatives exist to pickling for saving ML models?**\n",
        "\n",
        "Alternatives to pickling for saving ML models include:\n",
        "\n",
        "1. **Joblib**: Efficient for large models with numpy arrays.\n",
        "\n",
        "2. **ONNX**: Cross-platform format for different frameworks.\n",
        "\n",
        "3. **TensorFlow SavedModel**: For TensorFlow models.\n",
        "\n",
        "4. **HDF5**: Used with Keras for saving model architecture and weights.\n",
        "\n",
        "5. **MLflow**: Framework for managing and saving models with versioning.\n",
        "\n",
        "These provide better performance and compatibility based on the use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVfuNdAfIhYo"
      },
      "source": [
        "**22. What is heteroscedasticity, and why is it a problem?**\n",
        "\n",
        "   - **Heteroscedasticity** occurs when the residuals don’t have constant variance across predictor values (e.g., errors increase with larger values of predictors).\n",
        "\n",
        "   - Problems caused:\n",
        "\n",
        "     - Standard errors become biased, leading to invalid hypothesis testing.\n",
        "\n",
        "     - Predictions may still be accurate, but inferences (e.g., p-values) are unreliable.\n",
        "     \n",
        "   - Example: In a housing price model, errors may increase for higher-priced homes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy3c2-jjIhYq"
      },
      "source": [
        "**23. How can interaction terms enhance a regression model's predictive power?**\n",
        "\n",
        "   - **Interaction terms** account for the combined effect of two or more variables on the target.\n",
        "\n",
        "   - They allow the model to capture relationships that a simple additive model might miss.\n",
        "   - Example:\n",
        "   \n",
        "     - Suppose we have variables **\"hours studied\"** and **\"sleep hours\"**. The effect of studying on grades might depend on the amount of sleep.\n",
        "\n",
        "     - Adding an interaction term like **(hours studied × sleep hours)** helps capture this dependency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaAQhU_bIhYr"
      },
      "source": [
        "---\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}